{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import os\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as T\n",
    "import torchvision.models as M\n",
    "import copy\n",
    "import logging\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = \"mps\"\n",
    "else:\n",
    "    device = \"cpu\"\n",
    "\n",
    "# parameters\n",
    "n_epoch = 50\n",
    "batch_size = 1024\n",
    "lr = 5e-4\n",
    "input_dim = 2048\n",
    "output_dim = 1\n",
    "thresh = 0.5\n",
    "\n",
    "root = \"data/VehicleX/ReID Task/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_random_seed(seed=0):\n",
    "    # seed setting\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "set_random_seed()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the path and label of the dataset\n",
    "def path_generator(type):\n",
    "    paths = []\n",
    "    labels = []\n",
    "    # get the root path of the dataset\n",
    "    type_root = os.path.join(root, type)\n",
    "    finegrained_labels = os.listdir(type_root)\n",
    "    # loop each label\n",
    "    for label in finegrained_labels:\n",
    "        label_path = os.path.join(type_root, label)\n",
    "        paths.append(label_path)\n",
    "        labels.append(int(label.split(\"_\")[0])) \n",
    "    return paths, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VehicleDataset(Dataset):\n",
    "    def __init__(self, type, transform=None):\n",
    "        paths, labels = path_generator(type)\n",
    "        self.paths = paths\n",
    "        self.labels = labels\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        feature = Image.open(self.paths[idx])\n",
    "        label = self.labels[idx]\n",
    "        if self.transform:\n",
    "            feature = self.transform(feature)\n",
    "        return feature, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "transforms_src =  T.Compose(\n",
    "    [\n",
    "        T.RandomCrop(224),\n",
    "        T.RandomHorizontalFlip(),\n",
    "        T.ToTensor(),\n",
    "        T.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]\n",
    ")\n",
    "\n",
    "transforms_tar =  T.Compose(\n",
    "    [\n",
    "        T.Resize((224,224)),\n",
    "        T.ToTensor(),\n",
    "        T.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "src_data size:  45438\n",
      "tar_data size:  11579\n",
      "tar_test_data size:  1678\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([3, 224, 224])\n"
     ]
    }
   ],
   "source": [
    "src_data = VehicleDataset(\"train\", transforms_src)\n",
    "tar_data = VehicleDataset(\"gallery\", transforms_tar)\n",
    "tar_test_data = VehicleDataset(\"query\", transforms_tar)\n",
    "\n",
    "src_loader = DataLoader(src_data, batch_size, shuffle=True)\n",
    "tar_loader = DataLoader(tar_data, batch_size, shuffle=True)\n",
    "tar_test_loader = DataLoader(tar_test_data, batch_size, shuffle=True)\n",
    "\n",
    "# print size of each data\n",
    "print(\"src_data size: \", len(src_data))\n",
    "print(\"tar_data size: \", len(tar_data))\n",
    "print(\"tar_test_data size: \", len(tar_test_data))\n",
    "\n",
    "print(src_data[0][0].shape)\n",
    "print(tar_data[0][0].shape)\n",
    "print(tar_test_data[0][0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_distribution():\n",
    "    # check the label distribution of source\n",
    "    plt.subplot(1, 2, 1)\n",
    "    src_labels = []\n",
    "    for i in range(len(src_data)):\n",
    "        src_labels.append(src_data[i][1])\n",
    "    plt.hist(src_labels, bins=1362)\n",
    "\n",
    "    # check the label distribution of query\n",
    "    plt.subplot(1, 2, 2)\n",
    "    tar_labels = []\n",
    "    for i in range(len(tar_test_data)):\n",
    "        tar_labels.append(tar_test_data[i][1])\n",
    "    plt.hist(tar_labels, bins=1362)\n",
    "    plt.show()\n",
    "\n",
    "# show_distribution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# baseline model (ResNet34)\n",
    "class Baseline_ResNet34(nn.Module):\n",
    "    def __init__(self, output_dim):\n",
    "        super(Baseline_ResNet34, self).__init__()\n",
    "        self.resnet34 = M.resnet34(weights=M.ResNet34_Weights.DEFAULT)\n",
    "        self.feature_extractor = nn.Sequential(*list(self.resnet34.children())[:-1])\n",
    "        feat_dim = self.resnet34.fc.in_features\n",
    "        self.clf_fc = nn.Linear(feat_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.feature_extractor(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        clf = self.clf_fc(x)\n",
    "        return x, clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AverageMeter(object):\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def coral(source, target):\n",
    "    # my implementation of the original paper, the code is different, but the result is the same\n",
    "    d = source.data.shape[1]\n",
    "    ns, nt = source.data.shape[0], target.data.shape[0]\n",
    "    # source covariance\n",
    "    # calculating D'D for source and target\n",
    "    cov_s = source.T @ source\n",
    "    cov_t = target.T @ target\n",
    "\n",
    "    # divide D'D by (num-1)\n",
    "    cov_s = cov_s / (ns - 1)\n",
    "    cov_t = cov_t / (nt - 1)\n",
    "\n",
    "    # identity is a row vector of 1s\n",
    "    identity_s = torch.ones((1, ns), device=source.device)\n",
    "    identity_t = torch.ones((1, nt), device=target.device)\n",
    "\n",
    "    # calculate the mean of D per column\n",
    "    mean_s = identity_s @ source\n",
    "    mean_t = identity_t @ target\n",
    "\n",
    "    # calculate the squared mean\n",
    "    square_mean_s = mean_s.T @ mean_s\n",
    "    square_mean_t = mean_t.T @ mean_t\n",
    "\n",
    "    # divide squared mean by (num*(num-1))\n",
    "    square_mean_s = square_mean_s / (ns * (ns - 1))\n",
    "    square_mean_t = square_mean_t / (nt * (nt - 1))\n",
    "\n",
    "    # cov is (1/(num-1))*(D'*D) - (1/(num*(num-1)))*(mean)^T*(mean)\n",
    "    cov_s = cov_s - square_mean_s\n",
    "    cov_t = cov_t - square_mean_t\n",
    "\n",
    "    # cov_s - cov_t\n",
    "    diff = cov_s - cov_t\n",
    "\n",
    "    # loss = (1/4)*(1/(dim*dim))*square_norm\n",
    "    square_norm = torch.sum(torch.multiply(diff, diff))\n",
    "    loss = square_norm / (4 * d * d)\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train\n",
    "def train(\n",
    "        model, \n",
    "        src_loader, \n",
    "        tar_loader, \n",
    "        tar_test_loader, \n",
    "        optimizer, \n",
    "        args, \n",
    "):\n",
    "    logging.info(f\"Train info: lr: {lr}, batch_size: {batch_size}, n_epoch: {n_epoch}, thresh: {thresh}, optimizer: {optimizer}, criterion: {criterion}\")\n",
    "    best_acc = 0.0\n",
    "    best_model = copy.deepcopy(model.state_dict())\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    iter_src, iter_tar = iter(src_loader), iter(tar_loader)\n",
    "    for epoch in range(n_epoch):\n",
    "        # train\n",
    "        model.train()\n",
    "        train_loss_clf = AverageMeter()\n",
    "        train_loss_transfer = AverageMeter()\n",
    "        train_loss_total = AverageMeter()\n",
    "\n",
    "        for _ in range(batch_size):\n",
    "            data_src, label_src = next(iter_src)\n",
    "            data_tar, _ = next(iter_tar)\n",
    "            data_src, label_src = data_src.to(args.device), label_src.to(args.device)\n",
    "            data_tar = data_tar.to(args.device)\n",
    "\n",
    "            out_s, clf_s = model(data_src)\n",
    "            out_t, _ = model(data_tar)\n",
    "            clf_loss = criterion(clf_s, label_src)\n",
    "            transfer_loss = coral(out_s, out_t)\n",
    "            loss = clf_loss + args.transfer_loss_weight * transfer_loss\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss_clf.update(clf_loss.item())\n",
    "            train_loss_transfer.update(transfer_loss.item())\n",
    "            train_loss_total.update(loss.item())\n",
    "\n",
    "        # format in 4 decimal places\n",
    "        log = f\"Epoch: {epoch+1}/{n_epoch}, train_loss_clf: {train_loss_clf.avg:.4f}, \n",
    "                train_loss_transfer: {train_loss_transfer.avg:.4f}, \n",
    "                train_loss_total: {train_loss_total.avg:.4f},\"\n",
    "        \n",
    "        # test\n",
    "        acc, test_loss = test(model, tar_test_loader, args)\n",
    "        logging.info(f\"{log} test_acc: {acc:.4f}, test_loss: {test_loss:.4f}\")\n",
    "        if acc > best_acc:\n",
    "            best_acc = acc\n",
    "            best_model = copy.deepcopy(model.state_dict())\n",
    "    model.load_state_dict(best_model)        \n",
    "    return model\n",
    "\n",
    "# test\n",
    "def test(model, target_test_loader, args):\n",
    "    model.eval()\n",
    "    test_loss = AverageMeter()\n",
    "    correct = 0\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    len_target_dataset = len(target_test_loader.dataset)\n",
    "    with torch.no_grad():\n",
    "        for data, target in target_test_loader:\n",
    "            data, target = data.to(args.device), target.to(args.device)\n",
    "            _, clf = model.forward(data)\n",
    "            loss = criterion(clf, target)\n",
    "            test_loss.update(loss.item())\n",
    "            pred = torch.max(clf, 1)[1]\n",
    "            correct += torch.sum(pred == target)\n",
    "    acc = 100.0 * correct / len_target_dataset\n",
    "    return acc, test_loss.avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Baseline_ResNet34(output_dim)\n",
    "model = model.to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "train)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
