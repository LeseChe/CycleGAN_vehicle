{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import os\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as T\n",
    "import torchvision.models as M\n",
    "import copy\n",
    "import logging\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = \"mps\"\n",
    "else:\n",
    "    device = \"cpu\"\n",
    "\n",
    "# hyper parameters\n",
    "seed = 42\n",
    "n_epoch = 20\n",
    "batch_size = 16\n",
    "lr = 1e-4\n",
    "output_dim = 1362\n",
    "transfer_loss_weight = 0\n",
    "\n",
    "root = \"data/VehicleX/ReID Task/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parser.add_argument(\"--seed\", type=int, default=0)\n",
    "# parser.add_argument(\"--batch_size\", type=int, default=32)\n",
    "# parser.add_argument(\"--n_epoch\", type=int, default=20)\n",
    "# parser.add_argument(\"--lr\", type=float, default=1e-3)\n",
    "# args = parser.parse_args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_random_seed(seed=0):\n",
    "    # seed setting\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "set_random_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the path and label of the dataset\n",
    "def path_generator(type):\n",
    "    paths = []\n",
    "    labels = []\n",
    "    # get the root path of the dataset\n",
    "    type_root = os.path.join(root, type)\n",
    "    finegrained_labels = os.listdir(type_root)\n",
    "    # loop each label\n",
    "    for label in finegrained_labels:\n",
    "        label_path = os.path.join(type_root, label)\n",
    "        paths.append(label_path)\n",
    "        labels.append(int(label.split(\"_\")[0])) \n",
    "    return paths, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VehicleDataset(Dataset):\n",
    "    def __init__(self, type, transform=None):\n",
    "        paths, labels = path_generator(type)\n",
    "        self.paths = paths\n",
    "        self.labels = labels\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        feature = Image.open(self.paths[idx])\n",
    "        label = self.labels[idx]\n",
    "        if self.transform:\n",
    "            feature = self.transform(feature)\n",
    "        return feature, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "transforms_src =  T.Compose(\n",
    "    [\n",
    "        T.Resize((64,64)),\n",
    "        T.RandomHorizontalFlip(),\n",
    "        T.ToTensor(),\n",
    "        T.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]\n",
    ")\n",
    "\n",
    "transforms_tar =  T.Compose(\n",
    "    [\n",
    "        T.Resize((64,64)),\n",
    "        T.ToTensor(),\n",
    "        T.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "src_data size:  45438\n",
      "tar_data size:  11579\n",
      "tar_test_data size:  1678\n",
      "torch.Size([3, 64, 64])\n",
      "torch.Size([3, 64, 64])\n",
      "torch.Size([3, 64, 64])\n"
     ]
    }
   ],
   "source": [
    "src_data = VehicleDataset(\"train\", transforms_src)\n",
    "tar_data = VehicleDataset(\"gallery\", transforms_tar)\n",
    "tar_test_data = VehicleDataset(\"query\", transforms_tar)\n",
    "\n",
    "src_loader = DataLoader(src_data, batch_size, shuffle=True, num_workers=4)\n",
    "tar_loader = DataLoader(tar_data, batch_size, shuffle=True, num_workers=4)\n",
    "tar_test_loader = DataLoader(tar_test_data, batch_size, shuffle=True, num_workers=4)\n",
    "\n",
    "# print size of each data\n",
    "print(\"src_data size: \", len(src_data))\n",
    "print(\"tar_data size: \", len(tar_data))\n",
    "print(\"tar_test_data size: \", len(tar_test_data))\n",
    "\n",
    "print(src_data[0][0].shape)\n",
    "print(tar_data[0][0].shape)\n",
    "print(tar_test_data[0][0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_distribution():\n",
    "    # check the label distribution of source\n",
    "    plt.subplot(1, 2, 1)\n",
    "    src_labels = []\n",
    "    for i in range(len(src_data)):\n",
    "        src_labels.append(src_data[i][1])\n",
    "    plt.hist(src_labels, bins=1362)\n",
    "\n",
    "    # check the label distribution of query\n",
    "    plt.subplot(1, 2, 2)\n",
    "    tar_labels = []\n",
    "    for i in range(len(tar_test_data)):\n",
    "        tar_labels.append(tar_test_data[i][1])\n",
    "    plt.hist(tar_labels, bins=1362)\n",
    "    plt.show()\n",
    "\n",
    "# show_distribution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# baseline model (ResNet50)\n",
    "class Baseline_ResNet50(nn.Module):\n",
    "    def __init__(self, output_dim):\n",
    "        super(Baseline_ResNet50, self).__init__()\n",
    "        self.resnet50 = M.resnet50(weights=M.ResNet50_Weights.DEFAULT)\n",
    "        self.feature_extractor = nn.Sequential(*list(self.resnet50.children())[:-1])\n",
    "        feat_dim = self.resnet50.fc.in_features\n",
    "        self.clf_fc = nn.Linear(feat_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.feature_extractor(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        clf = self.clf_fc(x)\n",
    "        return x, clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AverageMeter(object):\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def coral(source, target):\n",
    "    # my implementation of the original paper, the code is different, but the result is the same\n",
    "    d = source.data.shape[1]\n",
    "    ns, nt = source.data.shape[0], target.data.shape[0]\n",
    "    # source covariance\n",
    "    # calculating D'D for source and target\n",
    "    cov_s = source.T @ source\n",
    "    cov_t = target.T @ target\n",
    "\n",
    "    # divide D'D by (num-1)\n",
    "    cov_s = cov_s / (ns - 1)\n",
    "    cov_t = cov_t / (nt - 1)\n",
    "\n",
    "    # identity is a row vector of 1s\n",
    "    identity_s = torch.ones((1, ns), device=source.device)\n",
    "    identity_t = torch.ones((1, nt), device=target.device)\n",
    "\n",
    "    # calculate the mean of D per column\n",
    "    mean_s = identity_s @ source\n",
    "    mean_t = identity_t @ target\n",
    "\n",
    "    # calculate the squared mean\n",
    "    square_mean_s = mean_s.T @ mean_s\n",
    "    square_mean_t = mean_t.T @ mean_t\n",
    "\n",
    "    # divide squared mean by (num*(num-1))\n",
    "    square_mean_s = square_mean_s / (ns * (ns - 1))\n",
    "    square_mean_t = square_mean_t / (nt * (nt - 1))\n",
    "\n",
    "    # cov is (1/(num-1))*(D'*D) - (1/(num*(num-1)))*(mean)^T*(mean)\n",
    "    cov_s = cov_s - square_mean_s\n",
    "    cov_t = cov_t - square_mean_t\n",
    "\n",
    "    # cov_s - cov_t\n",
    "    diff = cov_s - cov_t\n",
    "\n",
    "    # loss = (1/4)*(1/(dim*dim))*square_norm\n",
    "    square_norm = torch.sum(torch.multiply(diff, diff))\n",
    "    loss = square_norm / (4 * d * d)\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train\n",
    "def train(\n",
    "        model, \n",
    "        src_loader, \n",
    "        tar_loader, \n",
    "        tar_test_loader, \n",
    "        optimizer,\n",
    "        criterion\n",
    "):\n",
    "    best_acc = 0.0\n",
    "    # best_model = copy.deepcopy(model.state_dict())\n",
    "\n",
    "    for epoch in range(n_epoch):\n",
    "        model.train()\n",
    "        train_loss_clf = AverageMeter()\n",
    "        train_loss_transfer = AverageMeter()\n",
    "        train_loss_total = AverageMeter()\n",
    "\n",
    "        iter_src, iter_tar = iter(src_loader), iter(tar_loader)\n",
    "\n",
    "        for _ in range(batch_size):\n",
    "            data_src, label_src = next(iter_src)\n",
    "            data_tar, _ = next(iter_tar)\n",
    "            data_src, label_src = data_src.to(device), label_src.to(device)\n",
    "            data_tar = data_tar.to(device)\n",
    "\n",
    "            out_s, clf_s = model(data_src)\n",
    "            out_t, _ = model(data_tar)\n",
    "\n",
    "            clf_loss = criterion(clf_s, label_src)\n",
    "            transfer_loss = coral(out_s, out_t)\n",
    "            loss = clf_loss + transfer_loss_weight * transfer_loss\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss_clf.update(clf_loss.item())\n",
    "            train_loss_transfer.update(transfer_loss.item())\n",
    "            train_loss_total.update(loss.item())\n",
    "\n",
    "        # format in 4 decimal places\n",
    "        log = f\"Epoch: {epoch+1}/{n_epoch}, train_loss_clf: {train_loss_clf.avg:.4f}, \" \\\n",
    "              f\"train_loss_transfer: {train_loss_transfer.avg:.4f}, \" \\\n",
    "              f\"train_loss_total: {train_loss_total.avg:.4f}, \"\n",
    "        \n",
    "        # test\n",
    "        test_acc, test_loss = test(model, tar_test_loader)\n",
    "        logging.info(f\"{log} test_acc: {test_acc:.4f}, test_loss: {test_loss:.4f}\")\n",
    "        if test_acc > best_acc:\n",
    "            best_acc = test_acc\n",
    "            # best_model = copy.deepcopy(model.state_dict())\n",
    "    # model.load_state_dict(best_model)  \n",
    "    logging.info(f\"best_acc: {best_acc:.4f}\")      \n",
    "    return model\n",
    "\n",
    "# test\n",
    "def test(model, target_test_loader):\n",
    "    model.eval()\n",
    "    test_loss = AverageMeter()\n",
    "    correct = 0\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    len_target_dataset = len(target_test_loader.dataset)\n",
    "    with torch.no_grad():\n",
    "        for data, target in target_test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            _, clf = model.forward(data)\n",
    "            loss = criterion(clf, target)\n",
    "            test_loss.update(loss.item())\n",
    "            pred = torch.max(clf, 1)[1]\n",
    "            correct += torch.sum(pred == target)\n",
    "    acc = 100.0 * correct / len_target_dataset\n",
    "    return acc, test_loss.avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../aten/src/ATen/native/cuda/Loss.cu:242: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [9,0,0] Assertion `t >= 0 && t < n_classes` failed.\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: CUBLAS_STATUS_EXECUTION_FAILED when calling `cublasSgemm( handle, opa, opb, m, n, k, &alpha, a, lda, b, ldb, &beta, c, ldc)`",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/home/qixiang/COMP4660_ass2/working.ipynb Cell 14\u001b[0m line \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/home/qixiang/COMP4660_ass2/working.ipynb#X16sdnNjb2RlLXJlbW90ZQ%3D%3D?line=12'>13</a>\u001b[0m logging\u001b[39m.\u001b[39mbasicConfig(filename\u001b[39m=\u001b[39m\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39m./log/\u001b[39m\u001b[39m{\u001b[39;00mpath\u001b[39m}\u001b[39;00m\u001b[39m/\u001b[39m\u001b[39m{\u001b[39;00mfile_name\u001b[39m}\u001b[39;00m\u001b[39m.log\u001b[39m\u001b[39m'\u001b[39m, level\u001b[39m=\u001b[39mlogging\u001b[39m.\u001b[39mINFO)\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/home/qixiang/COMP4660_ass2/working.ipynb#X16sdnNjb2RlLXJlbW90ZQ%3D%3D?line=13'>14</a>\u001b[0m logging\u001b[39m.\u001b[39minfo(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mTrain info: lr: \u001b[39m\u001b[39m{\u001b[39;00mlr\u001b[39m}\u001b[39;00m\u001b[39m, batch_size: \u001b[39m\u001b[39m{\u001b[39;00mbatch_size\u001b[39m}\u001b[39;00m\u001b[39m, n_epoch: \u001b[39m\u001b[39m{\u001b[39;00mn_epoch\u001b[39m}\u001b[39;00m\u001b[39m, optimizer: \u001b[39m\u001b[39m{\u001b[39;00moptimizer\u001b[39m}\u001b[39;00m\u001b[39m, criterion: \u001b[39m\u001b[39m{\u001b[39;00mcriterion\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[0;32m---> <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/home/qixiang/COMP4660_ass2/working.ipynb#X16sdnNjb2RlLXJlbW90ZQ%3D%3D?line=15'>16</a>\u001b[0m train(model, src_loader, tar_loader, tar_test_loader, optimizer, criterion)\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/home/qixiang/COMP4660_ass2/working.ipynb#X16sdnNjb2RlLXJlbW90ZQ%3D%3D?line=17'>18</a>\u001b[0m torch\u001b[39m.\u001b[39msave(model\u001b[39m.\u001b[39mstate_dict(), \u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39m./log/\u001b[39m\u001b[39m{\u001b[39;00mpath\u001b[39m}\u001b[39;00m\u001b[39m/model\u001b[39m\u001b[39m{\u001b[39;00mfile_name\u001b[39m}\u001b[39;00m\u001b[39m.pth\u001b[39m\u001b[39m'\u001b[39m)\n",
      "\u001b[1;32m/home/qixiang/COMP4660_ass2/working.ipynb Cell 14\u001b[0m line \u001b[0;36m3\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/home/qixiang/COMP4660_ass2/working.ipynb#X16sdnNjb2RlLXJlbW90ZQ%3D%3D?line=31'>32</a>\u001b[0m loss \u001b[39m=\u001b[39m clf_loss \u001b[39m+\u001b[39m transfer_loss_weight \u001b[39m*\u001b[39m transfer_loss\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/home/qixiang/COMP4660_ass2/working.ipynb#X16sdnNjb2RlLXJlbW90ZQ%3D%3D?line=33'>34</a>\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[0;32m---> <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/home/qixiang/COMP4660_ass2/working.ipynb#X16sdnNjb2RlLXJlbW90ZQ%3D%3D?line=34'>35</a>\u001b[0m loss\u001b[39m.\u001b[39;49mbackward()\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/home/qixiang/COMP4660_ass2/working.ipynb#X16sdnNjb2RlLXJlbW90ZQ%3D%3D?line=35'>36</a>\u001b[0m optimizer\u001b[39m.\u001b[39mstep()\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/home/qixiang/COMP4660_ass2/working.ipynb#X16sdnNjb2RlLXJlbW90ZQ%3D%3D?line=37'>38</a>\u001b[0m train_loss_clf\u001b[39m.\u001b[39mupdate(clf_loss\u001b[39m.\u001b[39mitem())\n",
      "File \u001b[0;32m~/miniconda3/envs/cv/lib/python3.9/site-packages/torch/_tensor.py:488\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    478\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[1;32m    479\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    480\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[1;32m    481\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    486\u001b[0m         inputs\u001b[39m=\u001b[39minputs,\n\u001b[1;32m    487\u001b[0m     )\n\u001b[0;32m--> 488\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\n\u001b[1;32m    489\u001b[0m     \u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs\n\u001b[1;32m    490\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/envs/cv/lib/python3.9/site-packages/torch/autograd/__init__.py:197\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    192\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[1;32m    194\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    195\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    196\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 197\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    198\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[1;32m    199\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: CUBLAS_STATUS_EXECUTION_FAILED when calling `cublasSgemm( handle, opa, opb, m, n, k, &alpha, a, lda, b, ldb, &beta, c, ldc)`"
     ]
    }
   ],
   "source": [
    "model = Baseline_ResNet50(output_dim)\n",
    "model = model.to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "# optimizer = torch.optim.SGD(model.parameters(), lr=lr, momentum=0.9)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "path = f\"Baseline_experiment/Adam_b{batch_size}_lr{lr}\"\n",
    "file_name = f\"{time.strftime('%Y-%m-%d-%H-%M', time.localtime())}\"\n",
    "# logging with permission of creating new folder\n",
    "if not os.path.exists(f\"./log/{path}\"):\n",
    "    os.makedirs(f\"./log/{path}\")\n",
    "logging.basicConfig(filename=f'./log/{path}/{file_name}.log', level=logging.INFO)\n",
    "logging.info(f\"Train info: lr: {lr}, batch_size: {batch_size}, n_epoch: {n_epoch}, optimizer: {optimizer}, criterion: {criterion}\")\n",
    "\n",
    "train(model, src_loader, tar_loader, tar_test_loader, optimizer, criterion)\n",
    "\n",
    "torch.save(model.state_dict(), f'./log/{path}/model{file_name}.pth')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
